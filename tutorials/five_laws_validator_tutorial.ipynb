{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Laws Cognitive Governance Validator Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the Five Laws Validator tool to evaluate AI-generated content against SIM-ONE's Five Laws of Cognitive Governance.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Five Laws Validator evaluates text against:\n",
    "1. **Architectural Intelligence** - Intelligence from coordination, not brute force\n",
    "2. **Cognitive Governance** - Governed processes over unconstrained generation\n",
    "3. **Truth Foundation** - Absolute truth principles over probabilistic drift\n",
    "4. **Energy Stewardship** - Computational efficiency and resource awareness\n",
    "5. **Deterministic Reliability** - Consistent, predictable outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the tool\n",
    "TOOL_PATH = Path(\"../code/tools/run_five_laws_validator.py\")\n",
    "\n",
    "def validate_text(text, strictness=\"moderate\", format=\"json\"):\n",
    "    \"\"\"Helper function to validate text using the Five Laws Validator\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"python\", str(TOOL_PATH), \"--text\", text, \"--strictness\", strictness, \"--format\", format],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    return json.loads(result.stdout) if format == \"json\" else result.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Validation\n",
    "\n",
    "Let's validate a simple AI response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example AI response to validate\n",
    "response = \"\"\"Climate change is supported by overwhelming scientific evidence from multiple \n",
    "independent sources including atmospheric measurements, ice core data, and global temperature \n",
    "records. The consensus among climate scientists exceeds 97%.\"\"\"\n",
    "\n",
    "# Validate the response\n",
    "result = validate_text(response)\n",
    "\n",
    "# Display results\n",
    "print(f\"Overall Compliance: {result['scores']['overall_compliance']:.1f}%\")\n",
    "print(f\"Status: {result['pass_fail_status']}\")\n",
    "print(f\"\\nIndividual Law Scores:\")\n",
    "print(f\"  Law 1 (Architectural Intelligence): {result['scores']['law1_architectural_intelligence']:.1f}%\")\n",
    "print(f\"  Law 2 (Cognitive Governance): {result['scores']['law2_cognitive_governance']:.1f}%\")\n",
    "print(f\"  Law 3 (Truth Foundation): {result['scores']['law3_truth_foundation']:.1f}%\")\n",
    "print(f\"  Law 4 (Energy Stewardship): {result['scores']['law4_energy_stewardship']:.1f}%\")\n",
    "print(f\"  Law 5 (Deterministic Reliability): {result['scores']['law5_deterministic_reliability']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Validation with Different Strictness Levels\n",
    "\n",
    "The validator supports three strictness levels: lenient, moderate, and strict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"AI systems can be helpful tools for various tasks.\"\n",
    "\n",
    "for strictness in [\"lenient\", \"moderate\", \"strict\"]:\n",
    "    result = validate_text(response, strictness=strictness)\n",
    "    print(f\"\\n{strictness.upper()} Mode:\")\n",
    "    print(f\"  Compliance: {result['scores']['overall_compliance']:.1f}%\")\n",
    "    print(f\"  Status: {result['pass_fail_status']}\")\n",
    "    if result.get('violations'):\n",
    "        print(f\"  Violations: {len(result['violations'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Iterative Refinement Based on Recommendations\n",
    "\n",
    "When validation fails, the tool provides recommendations for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial response that may need refinement\n",
    "response = \"I think the answer is probably around 42, but I'm not entirely sure.\"\n",
    "\n",
    "result = validate_text(response, strictness=\"strict\")\n",
    "\n",
    "print(f\"Initial Compliance: {result['scores']['overall_compliance']:.1f}%\")\n",
    "print(f\"Status: {result['pass_fail_status']}\")\n",
    "\n",
    "if result.get('violations'):\n",
    "    print(f\"\\nViolations:\")\n",
    "    for violation in result['violations']:\n",
    "        print(f\"  - {violation}\")\n",
    "\n",
    "if result.get('recommendations'):\n",
    "    print(f\"\\nRecommendations:\")\n",
    "    for rec in result['recommendations']:\n",
    "        print(f\"  - {rec}\")\n",
    "\n",
    "if result.get('strengths'):\n",
    "    print(f\"\\nStrengths:\")\n",
    "    for strength in result['strengths']:\n",
    "        print(f\"  - {strength}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Batch Validation\n",
    "\n",
    "Validate multiple responses and compare their compliance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "    \"The Earth orbits the Sun due to gravitational forces, completing one orbit approximately every 365.25 days.\",\n",
    "    \"I believe the Earth probably goes around the Sun or something like that.\",\n",
    "    \"Based on astronomical observations and Newtonian mechanics, Earth's orbital period is 365.256 days.\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, response in enumerate(responses, 1):\n",
    "    result = validate_text(response)\n",
    "    results.append(result)\n",
    "    print(f\"\\nResponse {i}:\")\n",
    "    print(f\"  Compliance: {result['scores']['overall_compliance']:.1f}%\")\n",
    "    print(f\"  Status: {result['pass_fail_status']}\")\n",
    "    print(f\"  Truth Foundation Score: {result['scores']['law3_truth_foundation']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Using Summary Format\n",
    "\n",
    "The tool also supports a human-readable summary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"\"\"Machine learning models require careful validation to ensure they generalize \n",
    "well to unseen data. Cross-validation and holdout sets are standard practices.\"\"\"\n",
    "\n",
    "summary = validate_text(response, format=\"summary\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Command-Line Usage\n",
    "\n",
    "The tool can also be used directly from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using command line with text argument\n",
    "!python ../code/tools/run_five_laws_validator.py --text \"The speed of light is approximately 299,792,458 meters per second.\" --format summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stdin pipe\n",
    "!echo \"Water freezes at 0°C and boils at 100°C at standard atmospheric pressure.\" | python ../code/tools/run_five_laws_validator.py --format summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Integration with AI Response Pipeline\n",
    "\n",
    "Here's how to integrate the validator into an AI response generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_governed_response(prompt, max_iterations=3):\n",
    "    \"\"\"\n",
    "    Generate an AI response and validate it against Five Laws.\n",
    "    Refine until it passes or max iterations reached.\n",
    "    \"\"\"\n",
    "    # Simulated AI response generation (replace with actual AI model)\n",
    "    def generate_response(prompt):\n",
    "        return f\"Response to: {prompt}. This is a factual answer based on established scientific principles.\"\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        response = generate_response(prompt)\n",
    "        result = validate_text(response, strictness=\"moderate\")\n",
    "        \n",
    "        print(f\"\\nIteration {iteration + 1}:\")\n",
    "        print(f\"  Compliance: {result['scores']['overall_compliance']:.1f}%\")\n",
    "        print(f\"  Status: {result['pass_fail_status']}\")\n",
    "        \n",
    "        if result['pass_fail_status'] == \"PASS\":\n",
    "            print(f\"  ✅ Response passed governance validation!\")\n",
    "            return response, result\n",
    "        else:\n",
    "            print(f\"  ⚠️ Refinement needed\")\n",
    "            if result.get('recommendations'):\n",
    "                print(f\"  Recommendations: {result['recommendations'][0]}\")\n",
    "    \n",
    "    print(f\"\\n⚠️ Max iterations reached. Returning best attempt.\")\n",
    "    return response, result\n",
    "\n",
    "# Test the governed response pipeline\n",
    "prompt = \"Explain quantum entanglement\"\n",
    "final_response, validation = generate_governed_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Five Laws Validator is a powerful tool for ensuring AI-generated content meets cognitive governance standards. Key takeaways:\n",
    "\n",
    "- Use **moderate** strictness for general-purpose validation\n",
    "- Use **strict** mode for scientific, medical, or legal content\n",
    "- Use **lenient** mode for creative or exploratory content\n",
    "- Always review recommendations when validation fails\n",
    "- Integrate validation into your AI pipeline for consistent governance\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore other SIM-ONE tools: REP (Reasoning), ESL (Emotional State), VVP (Validation)\n",
    "- Learn about protocol composition for complex workflows\n",
    "- Read the full documentation at [PAPER2AGENT_INTEGRATION.md](../PAPER2AGENT_INTEGRATION.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

